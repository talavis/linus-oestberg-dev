[{"content":"Snacka Kubernetes is a series of webinars presented by Conoa. This is a summary of a webinar (in Swedish) I presented on November 22nd 2024.\nSUSE Security, formerly known as Neuvector, is an open-source container security platform intended especially for Kubernetes. In the text i will use the abbreviation NV. Like any other security platform, it\u0026rsquo;s not enough to just install it: it also requires customization and active monitoring over time.\nIt can be easily installed using a helm chart with the default settings, but I recommend that you check the values to harden the setup (the values file even contains multiple MUST be set for Rancher hardened cluster comments).\nIf you are using Rancher, you need to make the decision whether you want to use the Rancher-integrated version or the stand-alone version of NV. The main difference is in regard to how to authenticate: in one case you use Rancher, in the other you use local accounts, AD, OIDC, or SAML. Personally, I prefer to use the stand-alone version as it currently allows for more fine-grained RBAC management for different teams.\nThe first thing you should do after setting up a NV instance is to active auto-scanning. It means that the program will automatically scan both running containers and the cluster nodes, providing information about all known CVEs. NV can also set up automated register scanning, providing CVE information for each layer in the scanned image.\nAs can be expected from a container security program, it is possible to both list workloads/nodes with a CVE, as well as list CVEs in a workload/node. It is also possible to \u0026ldquo;accept\u0026rdquo; specific CVEs, both globally and at the workload level.\nOne of the main functionalities of NV is it\u0026rsquo;s runtime protection. It analyses two aspects for each workload (deployment, pod etc): network and processes. The protection is based on whitelisting the allowed traffic and processes, while alerting about or blocking anything not listed. In discovery mode, NV will consider any observed traffic or process as normal and it to the whitelist. Monitor and protect in turns alerts or blocks the processes and network trafic. The modes for network traffic and processes can be set separately. In my experience, the process protection works well, but someone needs to be responsible for keeping track of any changes to the behaviour of the workload to make sure it doesn\u0026rsquo;t suddently stop working because of blocked processes or network traffic. I believe this work should be performed by the (development) team responsible for the application. They are the ones who know the most about the application.\nThe rules for the runtime protection can be managed as a namespaced CRD that can be synchronised via e.g. a GitOps system.\nThere are a couple of different types of security events that are reported by NV. Of course, any violations of the runtime security rules are listed as security events, but there are also some other built-in rules detecting suspicios activity such as nmap runs inside a container. Apart from the security events, events are also generated for e.g. vulnerability scanning results.\nIt is possible to define response rules for any events, the available responses are to send a notification to e.g. a webhook, or to put a pod in quarantine. Quarantine means that any ingress or egress traffic to the pod will be blocked, and differs from the pod killing that is common in many other container security programs. By defining response rules, you can e.g. set any container containing a certain CVE in quarantine, making sure it cannot be exploited.\nWhen it comes to configuring a NV instance, there are a couple of choices avaialble. Of course, you can configure it using the GUI (frontend) but it\u0026rsquo;s also possible to set the config in the helm chart as config maps and secrets, but that config will only be applied when the NV controller pods have been scaled to 0 replicates. It is also possible to use federation to access and federate the configuration to multiple instances. Finally, each NV instance has an API with access to all configuration and information in the Neuvector instance, e.g. scanning results.\nPersonally, I prefer to use Infrastructure as Code, and the only efficient way I\u0026rsquo;ve found to apply configuration in a running NV instance has been to write some code that will interact with the API and apply the configuration that way.\n","permalink":"https://linus.oestberg.dev/posts/snacka-kubernetes-neuvector/","summary":"What to think of when running Neuvector in production.","title":"Snacka Kubernetes - Neuvector"},{"content":"Snacka Kubernetes is a series of webinars presented by Conoa. This is a summary of a webinar (in Swedish) I presented on Febuary 14th 2025. As it was live-streamed on February 14th, the demo had a simple Valentine theme and was advertised as \u0026ldquo;why you should always sign your love greetings\u0026rdquo;.\nMost people do not verify the source of their images. It means that anyone who get write access to the container registry can freely replace the tagged images with their own. Later, when e.g. a Kubernetes cluster pulls the image, the replaced image will be used in place of the real one.\nThe best way to verify that you use only the images you have built is to sign them. There are multiple ways of doing it, but Sigstore has been emerging as one of the major players. Sigstore consists of multiple parts, e.g. a tamper-proof ledger (Rekor) for signing certificates, and a command-line tool called Cosign to perform the actual signing.\nThe Cosign tool by default connects to a public Rekor instance, meaning that some metadata about a signed image is made publicly available if you sign your image. If a private key is used for the signature, the public key will be part of the record, while an OIDC token may contain more metadata. In the cases where this is a problem, you may want to host a private Sigstore setup. Some inspiration for how to set it up can be obtained by looking at Sigstore Scaffolding.\nWhen an image is signed, there will be a signing certificate stored in the Rekor instance and a signature in the container registry. It can be verified using an admission controller with support for Sigstore signatures, e.g. Kyverno. After configuring a cluster policy for verification of e.g. a key signature, any unsigned images will be rejected.\nWhile a signed image may protect from tampering with container registries, there are many other ways to attack a supply chain, see e.g. SLSA for an overview. SLSA itself is a framework for defining the trust in a build chain, but is not a central topic for this presentation.\nNo matter how secure the build and deploy chain is, it won\u0026rsquo;t matter if the code is manipulated. It is thus important to also sign any git commits as well so their origin can be verified.\nGit has no verification of the authenticity of the commiter. It means that anyone can pretend to be someone else and push commits in the other persons name (given that they are authorised to push to the repository). In order to verify the origin of the commits, they need to be signed. Git supports signing with GPG and SSH keys out of the box. Setting up commit signing with an SSH key is very straight-forward and only requires a few steps.\nA simple setup would be:\nssh-keygen # Fill in key password, location etc in the prompts git config --global gpg.format ssh git config --global user.signingkey ~/.ssh/keyname.pub Sign commits using git commit -S, or sign commits automatically by setting git config --global commit.gpgsign true.\nIf you use Github, you can associate the SSH public key with your account to get a \u0026ldquo;Verified\u0026rdquo; marker on your signed commits. Any incorrectly signed (or optionally also unsigned) commits will have an \u0026ldquo;unverified\u0026rdquo; marker.\n","permalink":"https://linus.oestberg.dev/posts/snacka-kubernetes-scs/","summary":"A short overview of image and git commit signing.","title":"Snacka Kubernetes - Supply Chain Security"},{"content":"About I\u0026rsquo;m originally a life science (bioinformatics) researcher, with a degree and PhD from Karolinska Institutet. I then moved on to working as a system developer (using Python) at Uppsala University/SciLifeLab, and have been involved in the development of e.g. SweFreq, the Swedish COVID-19 Data Portal (now Pathogens), and the SciLifeLab Data Delivery System. During my time at Uppsala University I moved on to Kubernetes operations and developed a strong interest in security. I\u0026rsquo;m now a Kubernetes consultant at Conoa AB, with a special focus on security in Kubernetes clusters.\nCertifications ","permalink":"https://linus.oestberg.dev/about/","summary":"\u003ch1 id=\"about\"\u003eAbout\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;m originally a life science (bioinformatics) \u003ca href=\"https://scholar.google.com/citations?user=nafzIpQAAAAJ\"\u003eresearcher\u003c/a\u003e, with a degree and PhD from Karolinska Institutet. I then moved on to working as a system developer (using Python) at Uppsala University/SciLifeLab, and have been involved in the development of e.g. \u003ca href=\"https://swefreq.nbis.se/\"\u003eSweFreq\u003c/a\u003e, the \u003ca href=\"https://www.pathogens.se/\"\u003eSwedish COVID-19 Data Portal (now Pathogens)\u003c/a\u003e, and the \u003ca href=\"https://delivery.scilifelab.se/\"\u003eSciLifeLab Data Delivery System\u003c/a\u003e. During my time at Uppsala University I moved on to Kubernetes operations and developed a strong interest in security. I\u0026rsquo;m now a Kubernetes consultant at \u003ca href=\"https://www.conoa.se/\"\u003eConoa AB\u003c/a\u003e, with a special focus on security in Kubernetes clusters.\u003c/p\u003e","title":""}]