[{"content":"During one of my assignments as a consultant, a team ended up with a misbehaving pod. It was perpetually stuck in the ContainerCreating state, and when deleted it instead got stuck in the Terminating state. When it had been in that state for more than 24 hours, my help was requested.\nThe examples below is a recreation done using my Rancher Desktop setup, it does not match the original setup.\nAs I started off with a pod in the Terminating state, my first thought was to investigate logs and to check whether there were any finalizers; nothing. Finally I logged into the node and manually deleted the container, which in turn killed the pod. Instead I got a new pod that got stuck in the ContainerCreatingstate. Killing that pod gave me a container stuck in the terminating state, and I was back where I started.\n$ kubectl get pods NAME READY STATUS RESTARTS AGE broken 0/1 ContainerCreating 0 5s $ kubectl delete pods broken $ kubectl get pods NAME READY STATUS RESTARTS AGE broken 0/1 Terminating 0 11m After again manually killing the container, I was back at the ContainerCreating state. As the pod was still starting, Kubernetes reported that no containers were, and thus no logs were available.\n$ kubectl get pods NAME READY STATUS RESTARTS AGE broken 0/1 ContainerCreating 0 3m34s $ kubectl logs broken Error from server (BadRequest): container \u0026#34;hooks-container\u0026#34; in pod \u0026#34;broken\u0026#34; is waiting to start: ContainerCreating Interestingly enough, the container was running, and logs were available when interacting with the container on the node:\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 18d35d234415 nginx \u0026#34;/docker-entrypoint.â€¦\u0026#34; 16 minutes ago Up 16 minutes k8s_hooks-container_broken_hooks_e54e57ac-0046-4a28-b380-63582239f086_0 /.../ $ docker logs 18d35d234415 /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /.../ At this stage, I started looking more thoroughly at the pod definition and a couple of rows caught my attention:\nspec: containers: - lifecycle: postStart: exec: command: ... preStop: exec: command: ... /.../ terminationGracePeriodSeconds: 432000 The pod was using container lifecycle hooks. These hooks are run at certain points in the container lifecycle. The postStart hook runs after the container has started, but stops the pod from reaching the state Running until it has finished. The preStop hook runs before the container is terminated, and may thus block pod termination. The preStop hook is overridden by terminationGracePeriodSeconds, so if the the grace period ends before the preStop command is finished, the pod/container will still terminate.\nIn the teams case some data failed to sync, and thus the postStart and preStop commands never terminated, blocking the pod start and termination. After the data was synced correctly, everything worked as intended.\nAs the hooks prevented the pod from starting and terminating, and also prevented the users from getting information about what was happening in the pod, they extended the time needed to remediate the issue by a lot. The manual intervention to check the container logs and to stop the containers also required intervention by admins, increasing the complexity as well. In order to avoid this type of issue, avoid handling/using the container lifecycle hooks or, if they truly are needed, make sure to not block pod creation and termination indefinitely, and finally make sure that the pod behaviour is well documented.\n","permalink":"https://linus.oestberg.dev/posts/container-lifecycle-hooks/","summary":"Container lifecycles hooks can make sure commands run before and after container creation. However, they may block container start and termination indefinitely, and you should be very careful when you use them.","title":"Container Lifecycle Hooks May Block Pod Creation and Termination"},{"content":"I recently renewed my CKS certification from CNCF. Sadly, the experience was everything but smooth.\nTested setups:\nMac Using a M2 Macbook Pro 13 with the latest 15.4.1 macOS.\nSmooth installation Unable to write any special characters (instead having to use the virtual keyboard) No graphical bugs in the PSI browser Linux Using a freshly installed Kubuntu 24.04 on a Dell XPS 13.\nManual installation Requires reconfiguration of AppArmor to avoid PSI Browser crashing during install: sysctl -w kernel.apparmor_restrict_unprivileged_userns=0 Works fine once installed Special characters work fine Graphical bugs for the environment All windows shifted (clicks ended up in unexpected places) Graphics corruption outside the active desktop Improved after resetting the desktop connection Network Do no use WIFI, no matter the quality. The latency is messing up all functionality, including copying values from the sidebar.\n","permalink":"https://linus.oestberg.dev/posts/lf-practical-exams/","summary":"Opinions about what setup to use for doing practical certification exams at Linux Foundation (CNCF).","title":"Practical Certification Exams at LF"},{"content":"Snacka Kubernetes is a series of webinars presented by Conoa. This is a summary of a webinar (in Swedish) I presented on November 22nd 2024.\nSUSE Security, formerly known as Neuvector, is an open-source container security platform intended especially for Kubernetes. In the text i will use the abbreviation NV. Like any other security platform, it\u0026rsquo;s not enough to just install it: it also requires customization and active monitoring over time.\nIt can be easily installed using a helm chart with the default settings, but I recommend that you check the values to harden the setup (the values file even contains multiple MUST be set for Rancher hardened cluster comments).\nIf you are using Rancher, you need to make the decision whether you want to use the Rancher-integrated version or the stand-alone version of NV. The main difference is in regard to how to authenticate: in one case you use Rancher, in the other you use local accounts, AD, OIDC, or SAML. Personally, I prefer to use the stand-alone version as it currently allows for more fine-grained RBAC management for different teams.\nThe first thing you should do after setting up a NV instance is to active auto-scanning. It means that the program will automatically scan both running containers and the cluster nodes, providing information about all known CVEs. NV can also set up automated register scanning, providing CVE information for each layer in the scanned image.\nAs can be expected from a container security program, it is possible to both list workloads/nodes with a CVE, as well as list CVEs in a workload/node. It is also possible to \u0026ldquo;accept\u0026rdquo; specific CVEs, both globally and at the workload level.\nOne of the main functionalities of NV is it\u0026rsquo;s runtime protection. It analyses two aspects for each workload (deployment, pod etc): network and processes. The protection is based on whitelisting the allowed traffic and processes, while alerting about or blocking anything not listed. In discovery mode, NV will consider any observed traffic or process as normal and it to the whitelist. Monitor and protect in turns alerts or blocks the processes and network trafic. The modes for network traffic and processes can be set separately. In my experience, the process protection works well, but someone needs to be responsible for keeping track of any changes to the behaviour of the workload to make sure it doesn\u0026rsquo;t suddently stop working because of blocked processes or network traffic. I believe this work should be performed by the (development) team responsible for the application. They are the ones who know the most about the application.\nThe rules for the runtime protection can be managed as a namespaced CRD that can be synchronised via e.g. a GitOps system.\nThere are a couple of different types of security events that are reported by NV. Of course, any violations of the runtime security rules are listed as security events, but there are also some other built-in rules detecting suspicios activity such as nmap runs inside a container. Apart from the security events, events are also generated for e.g. vulnerability scanning results.\nIt is possible to define response rules for any events, the available responses are to send a notification to e.g. a webhook, or to put a pod in quarantine. Quarantine means that any ingress or egress traffic to the pod will be blocked, and differs from the pod killing that is common in many other container security programs. By defining response rules, you can e.g. set any container containing a certain CVE in quarantine, making sure it cannot be exploited.\nWhen it comes to configuring a NV instance, there are a couple of choices avaialble. Of course, you can configure it using the GUI (frontend) but it\u0026rsquo;s also possible to set the config in the helm chart as config maps and secrets, but that config will only be applied when the NV controller pods have been scaled to 0 replicates. It is also possible to use federation to access and federate the configuration to multiple instances. Finally, each NV instance has an API with access to all configuration and information in the Neuvector instance, e.g. scanning results.\nPersonally, I prefer to use Infrastructure as Code, and the only efficient way I\u0026rsquo;ve found to apply configuration in a running NV instance has been to write some code that will interact with the API and apply the configuration that way.\n","permalink":"https://linus.oestberg.dev/posts/snacka-kubernetes-neuvector/","summary":"What to think of when running Neuvector in production.","title":"Snacka Kubernetes - Neuvector"},{"content":"Snacka Kubernetes is a series of webinars presented by Conoa. This is a summary of a webinar (in Swedish) I presented on Febuary 14th 2025. As it was live-streamed on February 14th, the demo had a simple Valentine theme and was advertised as \u0026ldquo;why you should always sign your love greetings\u0026rdquo;.\nMost people do not verify the source of their images. It means that anyone who get write access to the container registry can freely replace the tagged images with their own. Later, when e.g. a Kubernetes cluster pulls the image, the replaced image will be used in place of the real one.\nThe best way to verify that you use only the images you have built is to sign them. There are multiple ways of doing it, but Sigstore has been emerging as one of the major players. Sigstore consists of multiple parts, e.g. a tamper-proof ledger (Rekor) for signing certificates, and a command-line tool called Cosign to perform the actual signing.\nThe Cosign tool by default connects to a public Rekor instance, meaning that some metadata about a signed image is made publicly available if you sign your image. If a private key is used for the signature, the public key will be part of the record, while an OIDC token may contain more metadata. In the cases where this is a problem, you may want to host a private Sigstore setup. Some inspiration for how to set it up can be obtained by looking at Sigstore Scaffolding.\nWhen an image is signed, there will be a signing certificate stored in the Rekor instance and a signature in the container registry. It can be verified using an admission controller with support for Sigstore signatures, e.g. Kyverno. After configuring a cluster policy for verification of e.g. a key signature, any unsigned images will be rejected.\nWhile a signed image may protect from tampering with container registries, there are many other ways to attack a supply chain, see e.g. SLSA for an overview. SLSA itself is a framework for defining the trust in a build chain, but is not a central topic for this presentation.\nNo matter how secure the build and deploy chain is, it won\u0026rsquo;t matter if the code is manipulated. It is thus important to also sign any git commits as well so their origin can be verified.\nGit has no verification of the authenticity of the commiter. It means that anyone can pretend to be someone else and push commits in the other persons name (given that they are authorised to push to the repository). In order to verify the origin of the commits, they need to be signed. Git supports signing with GPG and SSH keys out of the box. Setting up commit signing with an SSH key is very straight-forward and only requires a few steps.\nA simple setup would be:\nssh-keygen # Fill in key password, location etc in the prompts git config --global gpg.format ssh git config --global user.signingkey ~/.ssh/keyname.pub Sign commits using git commit -S, or sign commits automatically by setting git config --global commit.gpgsign true.\nIf you use Github, you can associate the SSH public key with your account to get a \u0026ldquo;Verified\u0026rdquo; marker on your signed commits. Any incorrectly signed (or optionally also unsigned) commits will have an \u0026ldquo;unverified\u0026rdquo; marker.\n","permalink":"https://linus.oestberg.dev/posts/snacka-kubernetes-scs/","summary":"A short overview of image and git commit signing.","title":"Snacka Kubernetes - Supply Chain Security"},{"content":"About I\u0026rsquo;m originally a life science (bioinformatics) researcher, with a degree and PhD from Karolinska Institutet. I then moved on to working as a system developer (using Python) at Uppsala University/SciLifeLab, and have been involved in the development of e.g. SweFreq, the Swedish COVID-19 Data Portal (now Pathogens), and the SciLifeLab Data Delivery System. During my time at Uppsala University I moved on to Kubernetes operations and developed a strong interest in security. I\u0026rsquo;m now a Kubernetes consultant at Conoa AB, with a special focus on security in Kubernetes clusters.\nCertifications ","permalink":"https://linus.oestberg.dev/about/","summary":"\u003ch1 id=\"about\"\u003eAbout\u003c/h1\u003e\n\u003cp\u003eI\u0026rsquo;m originally a life science (bioinformatics) \u003ca href=\"https://scholar.google.com/citations?user=nafzIpQAAAAJ\"\u003eresearcher\u003c/a\u003e, with a degree and PhD from Karolinska Institutet. I then moved on to working as a system developer (using Python) at Uppsala University/SciLifeLab, and have been involved in the development of e.g. \u003ca href=\"https://swefreq.nbis.se/\"\u003eSweFreq\u003c/a\u003e, the \u003ca href=\"https://www.pathogens.se/\"\u003eSwedish COVID-19 Data Portal (now Pathogens)\u003c/a\u003e, and the \u003ca href=\"https://delivery.scilifelab.se/\"\u003eSciLifeLab Data Delivery System\u003c/a\u003e. During my time at Uppsala University I moved on to Kubernetes operations and developed a strong interest in security. I\u0026rsquo;m now a Kubernetes consultant at \u003ca href=\"https://www.conoa.se/\"\u003eConoa AB\u003c/a\u003e, with a special focus on security in Kubernetes clusters.\u003c/p\u003e","title":""}]